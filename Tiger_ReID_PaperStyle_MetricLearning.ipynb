{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d91d864",
   "metadata": {},
   "source": [
    "# 🐯 Tiger Re-Identification — Metric Learning (Paper-Style Eval, DINOv2)\n",
    "\n",
    "This notebook implements a **metric-learning** pipeline for tiger re-ID with **paper-style evaluation**:\n",
    "\n",
    "- **Train/val split** is **disjoint by identity** (no ID leakage).\n",
    "- **Eval within val**: either **1 random gallery image per ID** (as used in many re-ID papers) **or** deterministic **first-per-ID** (for reproducibility).\n",
    "- Backbone: **DINOv2 ViT** via `torch.hub` wrapped in **TigerDINO**.\n",
    "- Training: **Balanced P×K sampler**, **batch-hard triplet** on **L2-normalized embeddings**.\n",
    "- Metrics: **Rank-1** and **mAP** computed by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a895eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T08:48:28.127812Z",
     "iopub.status.busy": "2025-08-12T08:48:28.127656Z",
     "iopub.status.idle": "2025-08-12T08:48:28.131664Z",
     "shell.execute_reply": "2025-08-12T08:48:28.131204Z",
     "shell.execute_reply.started": "2025-08-12T08:48:28.127797Z"
    }
   },
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Config\n",
    "# =====================\n",
    "CSV_PATH   = \"./training_annotation/reid_list_train.csv\"  # columns: tiger_id, image_filename\n",
    "IMAGE_ROOT = \"./train\"  # where image files live\n",
    "\n",
    "SEED       = 42\n",
    "VAL_RATIO  = 0.2         # split-by-ID\n",
    "IMG_SIZE   = 224\n",
    "\n",
    "# Backbone / Head\n",
    "DINO_MODEL = \"dinov2_vitb14\"\n",
    "EMBED_DIM  = 512\n",
    "\n",
    "# PK Sampler & Optim\n",
    "P, K       = 8, 4\n",
    "EPOCHS     = 20\n",
    "LR         = 1e-4\n",
    "WD         = 5e-2\n",
    "MARGIN     = 0.3\n",
    "BATCH_EVAL = 64\n",
    "NUM_WORKERS= 4\n",
    "SAVE_DIR   = \"checkpoints\"\n",
    "\n",
    "# Evaluation mode inside VAL:\n",
    "# If True -> \"paper style\": one RANDOM gallery image per ID (seeded)\n",
    "# If False -> deterministic: first-per-ID goes to QUERY, rest in GALLERY (reliable across runs)\n",
    "PAPER_RANDOM_GALLERY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d42f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Total images: 1887 | Unique tigers: 107\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Imports & setup\n",
    "# =====================\n",
    "import os, math, random\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "def seed_all(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "seed_all(SEED)\n",
    "\n",
    "assert os.path.exists(CSV_PATH), f\"CSV not found: {CSV_PATH}\"\n",
    "df = pd.read_csv(CSV_PATH, header=None, names=[\"tiger_id\", \"image_filename\"])\n",
    "print(f\"Total images: {len(df)} | Unique tigers: {df['tiger_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb98f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IDs: 86, Val IDs: 21\n",
      "Train images: 1428, Val images: 459\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Split-by-ID into train / val (CLOSED-SET eval is within VAL only)\n",
    "# =====================\n",
    "uniq_ids = df[\"tiger_id\"].unique()\n",
    "rng = np.random.default_rng(SEED)\n",
    "rng.shuffle(uniq_ids)\n",
    "n_val = max(1, int(len(uniq_ids) * VAL_RATIO))\n",
    "val_ids = set(uniq_ids[:n_val])\n",
    "train_ids = set(uniq_ids[n_val:])\n",
    "\n",
    "train_df = df[df[\"tiger_id\"].isin(train_ids)].reset_index(drop=True)\n",
    "val_df   = df[df[\"tiger_id\"].isin(val_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train IDs: {len(train_ids)}, Val IDs: {len(val_ids)}\")\n",
    "print(f\"Train images: {len(train_df)}, Val images: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85416af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Transforms\n",
    "# =====================\n",
    "from torchvision import transforms\n",
    "def make_transforms(img_size=IMG_SIZE):\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.2)),\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.6,1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.2)),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "train_tf, val_tf = make_transforms(IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad7dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Dataset with SHARED ID MAP (consistent labels everywhere)\n",
    "# =====================\n",
    "class TigerCSVReID(Dataset):\n",
    "    def __init__(self, dataframe, image_root, transform=None, id_map=None):\n",
    "        self.df = dataframe.copy().reset_index(drop=True)\n",
    "        self.image_root = Path(image_root)\n",
    "        self.transform = transform\n",
    "\n",
    "        if id_map is None:\n",
    "            uniq = sorted(self.df[\"tiger_id\"].unique().tolist())\n",
    "            self.id2idx = {tid:i for i,tid in enumerate(uniq)}\n",
    "        else:\n",
    "            self.id2idx = id_map  # shared mapping across train/val/gallery/query\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "        img_path = self.image_root / str(r[\"image_filename\"])\n",
    "        with Image.open(img_path) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        seq_id = self.id2idx[r[\"tiger_id\"]]\n",
    "        return im, torch.tensor(seq_id, dtype=torch.long), r[\"tiger_id\"], str(img_path)\n",
    "\n",
    "# Global mapping so labels line up across datasets\n",
    "ALL_IDS = sorted(df[\"tiger_id\"].unique().tolist())\n",
    "IDMAP = {tid:i for i,tid in enumerate(ALL_IDS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "066ffbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Balanced P×K Sampler (for training)\n",
    "# =====================\n",
    "from collections import defaultdict\n",
    "\n",
    "class BalancedPKSampler(Sampler):\n",
    "    def __init__(self, dataset, P=8, K=4, id_column=\"tiger_id\"):\n",
    "        self.dataset = dataset\n",
    "        self.P, self.K = int(P), int(K)\n",
    "\n",
    "        lbl2idxs = defaultdict(list)\n",
    "        for i in range(len(dataset)):\n",
    "            r = dataset.df.iloc[i]\n",
    "            seq = dataset.id2idx[r[id_column]]\n",
    "            lbl2idxs[seq].append(i)\n",
    "        self.lbl2idxs = dict(lbl2idxs)\n",
    "        self.labels = list(self.lbl2idxs.keys())\n",
    "        self.batch_size = self.P*self.K\n",
    "        total = sum(len(v) for v in self.lbl2idxs.values())\n",
    "        self.num_batches = max(1, math.ceil(total/self.batch_size))\n",
    "\n",
    "    def __len__(self): return self.num_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        labels = self.labels[:]\n",
    "        for _ in range(self.num_batches):\n",
    "            chosen = random.sample(labels, k=min(self.P, len(labels)))\n",
    "            batch = []\n",
    "            for lab in chosen:\n",
    "                idxs = self.lbl2idxs[lab]\n",
    "                if len(idxs) >= self.K:\n",
    "                    batch.extend(random.sample(idxs, self.K))\n",
    "                else:\n",
    "                    batch.extend(random.choices(idxs, k=self.K))\n",
    "            random.shuffle(batch)\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69bf9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Triplet (batch-hard) with cosine distance\n",
    "# =====================\n",
    "def pairwise_cosine_distance(emb):\n",
    "    return 1.0 - (emb @ emb.t())\n",
    "\n",
    "def batch_hard_triplet(labels, emb, margin=0.3):\n",
    "    with torch.no_grad():\n",
    "        labels = labels.view(-1,1)\n",
    "        matches = (labels == labels.t())\n",
    "        eye = torch.eye(matches.size(0), dtype=torch.bool, device=matches.device)\n",
    "        pos_mask = matches & ~eye\n",
    "        neg_mask = ~matches\n",
    "\n",
    "    dist = pairwise_cosine_distance(emb)\n",
    "    pos_d = dist.clone(); pos_d[~pos_mask] = -1.0\n",
    "    neg_d = dist.clone(); neg_d[~neg_mask] =  2.0\n",
    "\n",
    "    hardest_pos = pos_d.max(dim=1).values\n",
    "    hardest_neg = neg_d.min(dim=1).values\n",
    "    valid = hardest_pos >= 0.0\n",
    "    if valid.sum() == 0:\n",
    "        return emb.new_tensor(0.0, requires_grad=True)\n",
    "    return F.relu(hardest_pos[valid] - hardest_neg[valid] + margin).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0533235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# DINOv2 backbone wrapper (TigerDINO)\n",
    "# =====================\n",
    "class TigerDINO(nn.Module):\n",
    "    def __init__(self, dino_model=\"dinov2_vitb14\", embed_dim=512, num_classes=None, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone, feat_dim = self._load_dinov2(dino_model, pretrained)\n",
    "        self.embedding = nn.Linear(feat_dim, embed_dim)\n",
    "        self.bn = nn.BatchNorm1d(embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes) if num_classes is not None else None\n",
    "\n",
    "    def _load_dinov2(self, name, pretrained=True):\n",
    "        try:\n",
    "            model = torch.hub.load('facebookresearch/dinov2', name, trust_repo=True)\n",
    "            feat_dim = getattr(model, \"embed_dim\", 768)\n",
    "            return model, feat_dim\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not load DINOv2 via torch.hub ({e}). Using a small fallback convnet.\")\n",
    "            fallback = nn.Sequential(\n",
    "                nn.Conv2d(3,64,3,2,1), nn.ReLU(),\n",
    "                nn.Conv2d(64,128,3,2,1), nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "            )\n",
    "            return fallback, 128\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        h = self.backbone(x)\n",
    "        if h.ndim > 2:  # fallback path\n",
    "            h = torch.flatten(h, 1)\n",
    "        z = self.bn(self.embedding(h))\n",
    "        if return_features or self.classifier is None:\n",
    "            return z\n",
    "        logits = self.classifier(z)\n",
    "        return logits, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c79c0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BalancedPKSampler: P=8, K=4, batch=32\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Build training datasets/loaders\n",
    "# =====================\n",
    "train_ds = TigerCSVReID(train_df, IMAGE_ROOT, transform=train_tf, id_map=IDMAP)\n",
    "val_ds   = TigerCSVReID(val_df,   IMAGE_ROOT, transform=val_tf,   id_map=IDMAP)\n",
    "\n",
    "try:\n",
    "    train_loader = DataLoader(train_ds, batch_sampler=BalancedPKSampler(train_ds, P=P, K=K, id_column=\"tiger_id\"),\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Using BalancedPKSampler: P={P}, K={K}, batch={P*K}\")\n",
    "except Exception as e:\n",
    "    print(\"BalancedPKSampler failed; falling back to simple batching:\", e)\n",
    "    train_loader = DataLoader(train_ds, batch_size=P*K, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c46a6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gallery size: 21 Query size: 438 | #IDs: 21\n"
     ]
    }
   ],
   "source": [
    "# --- Build gallery/query FROM VAL (paper-style or deterministic) ---\n",
    "def build_val_gallery_query(val_df, random_one_per_id=True, seed=42):\n",
    "    \"\"\"\n",
    "    Paper style (default):\n",
    "      - For each identity in VAL with >=2 images:\n",
    "          pick 1 image at random for the GALLERY (seeded),\n",
    "          put the remaining images in the QUERY.\n",
    "    Deterministic alternative (random_one_per_id=False):\n",
    "      - Use the first image (after sorting) as GALLERY, rest as QUERY.\n",
    "    \"\"\"\n",
    "    # Keep only IDs that have at least 2 images in VAL\n",
    "    counts = val_df[\"tiger_id\"].value_counts()\n",
    "    ids_keep = set(counts[counts >= 2].index)\n",
    "    v = val_df[val_df[\"tiger_id\"].isin(ids_keep)].copy()\n",
    "\n",
    "    # Give every row a stable unique row id we can sample on\n",
    "    v = v.reset_index(drop=False).rename(columns={\"index\": \"orig_idx\"})\n",
    "\n",
    "    if random_one_per_id:\n",
    "        # one RANDOM gallery per ID (seeded) -> rest are query\n",
    "        gal_ix = (v.groupby(\"tiger_id\")[\"orig_idx\"]\n",
    "                    .apply(lambda s: s.sample(n=1, random_state=seed))\n",
    "                    .tolist())\n",
    "    else:\n",
    "        # deterministic: first-per-ID (after sorting) goes to gallery\n",
    "        v = v.sort_values([\"tiger_id\", \"image_filename\"]).reset_index(drop=True)\n",
    "        gal_ix = (v.groupby(\"tiger_id\")[\"orig_idx\"]\n",
    "                    .first()\n",
    "                    .tolist())\n",
    "\n",
    "    mask = v[\"orig_idx\"].isin(gal_ix)\n",
    "    gal_df = v.loc[mask, [\"tiger_id\", \"image_filename\"]].reset_index(drop=True)\n",
    "    qry_df = v.loc[~mask, [\"tiger_id\", \"image_filename\"]].reset_index(drop=True)\n",
    "    return gal_df, qry_df\n",
    "gal_df, qry_df = build_val_gallery_query(val_df, random_one_per_id=PAPER_RANDOM_GALLERY, seed=SEED)\n",
    "print(\"Gallery size:\", len(gal_df), \"Query size:\", len(qry_df), \"| #IDs:\", qry_df[\"tiger_id\"].nunique())\n",
    "\n",
    "gallery_ds = TigerCSVReID(gal_df, IMAGE_ROOT, transform=val_tf, id_map=IDMAP)\n",
    "query_ds   = TigerCSVReID(qry_df, IMAGE_ROOT, transform=val_tf, id_map=IDMAP)\n",
    "\n",
    "gallery_loader = DataLoader(gallery_ds, batch_size=BATCH_EVAL, shuffle=False,\n",
    "                            num_workers=NUM_WORKERS, pin_memory=True)\n",
    "query_loader   = DataLoader(query_ds,   batch_size=BATCH_EVAL, shuffle=False,\n",
    "                            num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7334ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Embedding & evaluation (Rank-1, mAP)\n",
    "# =====================\n",
    "@torch.no_grad()\n",
    "def embed_loader(model, loader, device):\n",
    "    model.eval()\n",
    "    feats, ids = [], []\n",
    "    for imgs, seq_ids, raw_ids, _ in loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        f = F.normalize(model(imgs, return_features=True), dim=1)\n",
    "        feats.append(f.cpu())\n",
    "        ids.append(seq_ids.cpu())\n",
    "    return torch.cat(feats, 0), torch.cat(ids, 0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def reid_eval(model, gallery_loader, query_loader, device):\n",
    "    g_feats, g_ids = embed_loader(model, gallery_loader, device)\n",
    "    q_feats, q_ids = embed_loader(model, query_loader, device)\n",
    "\n",
    "    sims = q_feats @ g_feats.t()  # cosine\n",
    "\n",
    "    # Rank-1\n",
    "    nn_idx = sims.argmax(dim=1)\n",
    "    nn_ids = g_ids[nn_idx]\n",
    "    rank1 = (nn_ids == q_ids).float().mean().item() * 100.0\n",
    "\n",
    "    # mAP\n",
    "    ap_list = []\n",
    "    gid_to_pos = {}\n",
    "    for i, gid in enumerate(g_ids.tolist()):\n",
    "        gid_to_pos.setdefault(gid, []).append(i)\n",
    "\n",
    "    for q in range(q_feats.size(0)):\n",
    "        gt = q_ids[q].item()\n",
    "        order = torch.argsort(sims[q], descending=True)\n",
    "        relevant = set(gid_to_pos.get(gt, []))\n",
    "        if not relevant:\n",
    "            continue\n",
    "        hits = 0; precisions = []\n",
    "        for rank, gidx in enumerate(order.tolist(), start=1):\n",
    "            if gidx in relevant:\n",
    "                hits += 1\n",
    "                precisions.append(hits / rank)\n",
    "        if precisions:\n",
    "            ap_list.append(sum(precisions) / len(relevant))\n",
    "\n",
    "    mAP = (sum(ap_list)/len(ap_list)*100.0) if ap_list else 0.0\n",
    "    return rank1, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0399caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sagemaker-user/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Epoch 1/20: 100%|██████████| 45/45 [00:17<00:00,  2.63it/s, loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 01 | loss=0.1227 | Rank-1=82.88% | mAP=88.79%\n",
      "💾 Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 45/45 [00:16<00:00,  2.77it/s, loss=0.0531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 02 | loss=0.0531 | Rank-1=86.07% | mAP=90.95%\n",
      "💾 Saved new best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 45/45 [00:16<00:00,  2.77it/s, loss=0.0452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 03 | loss=0.0452 | Rank-1=87.21% | mAP=90.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 45/45 [00:16<00:00,  2.77it/s, loss=0.0331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 04 | loss=0.0331 | Rank-1=80.82% | mAP=87.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 45/45 [00:16<00:00,  2.77it/s, loss=0.0321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 05 | loss=0.0321 | Rank-1=85.16% | mAP=89.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 45/45 [00:16<00:00,  2.77it/s, loss=0.0187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 06 | loss=0.0187 | Rank-1=86.30% | mAP=89.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 45/45 [00:16<00:00,  2.76it/s, loss=0.0148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 07 | loss=0.0148 | Rank-1=85.39% | mAP=88.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 45/45 [00:16<00:00,  2.77it/s, loss=0.0133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 08 | loss=0.0133 | Rank-1=86.53% | mAP=89.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 45/45 [00:16<00:00,  2.76it/s, loss=0.0114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 09 | loss=0.0114 | Rank-1=86.76% | mAP=89.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 45/45 [00:16<00:00,  2.78it/s, loss=0.0115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 10 | loss=0.0115 | Rank-1=85.39% | mAP=89.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 45/45 [00:16<00:00,  2.76it/s, loss=0.00879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 11 | loss=0.0088 | Rank-1=85.16% | mAP=89.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 45/45 [00:16<00:00,  2.79it/s, loss=0.00577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 12 | loss=0.0058 | Rank-1=85.62% | mAP=89.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 45/45 [00:16<00:00,  2.77it/s, loss=0.0143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 13 | loss=0.0143 | Rank-1=86.07% | mAP=90.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 45/45 [00:16<00:00,  2.76it/s, loss=0.00955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 14 | loss=0.0096 | Rank-1=85.39% | mAP=88.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 45/45 [00:16<00:00,  2.77it/s, loss=0.0112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 15 | loss=0.0112 | Rank-1=82.88% | mAP=87.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 45/45 [00:16<00:00,  2.77it/s, loss=0.0072] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 16 | loss=0.0072 | Rank-1=85.84% | mAP=90.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 45/45 [00:16<00:00,  2.76it/s, loss=0.00675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 17 | loss=0.0068 | Rank-1=83.11% | mAP=87.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 45/45 [00:16<00:00,  2.78it/s, loss=0.00863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 18 | loss=0.0086 | Rank-1=81.96% | mAP=86.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 45/45 [00:16<00:00,  2.78it/s, loss=0.00365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Epoch 19 | loss=0.0036 | Rank-1=84.25% | mAP=88.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:  93%|█████████▎| 42/45 [00:15<00:01,  2.87it/s, loss=0.0091] "
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Train loop + eval\n",
    "# =====================\n",
    "from pathlib import Path\n",
    "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model = TigerDINO(dino_model=DINO_MODEL, embed_dim=EMBED_DIM, num_classes=None, pretrained=True).to(device)\n",
    "\n",
    "bb_params, head_params = [], []\n",
    "for n,p in model.named_parameters():\n",
    "    if not p.requires_grad: \n",
    "        continue\n",
    "    (bb_params if \"backbone\" in n else head_params).append(p)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": bb_params,  \"lr\": LR*0.1},\n",
    "    {\"params\": head_params,\"lr\": LR},\n",
    "], weight_decay=WD)\n",
    "\n",
    "best_map = -1.0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running, steps = 0.0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for imgs, labels, *_ in pbar:\n",
    "        imgs   = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        feats = model(imgs, return_features=True)\n",
    "        feats = F.normalize(feats, dim=1)\n",
    "        loss  = batch_hard_triplet(labels, feats, margin=MARGIN)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += float(loss.item()); steps += 1\n",
    "        pbar.set_postfix(loss=running/max(1,steps))\n",
    "\n",
    "    train_loss = running / max(1,steps)\n",
    "    rank1, mAP = reid_eval(model, gallery_loader, query_loader, device=device)\n",
    "    print(f\"[Val] Epoch {epoch:02d} | loss={train_loss:.4f} | Rank-1={rank1:.2f}% | mAP={mAP:.2f}%\")\n",
    "\n",
    "    ckpt = {\"model\": model.state_dict(), \"epoch\": epoch, \"mAP\": mAP}\n",
    "    torch.save(ckpt, os.path.join(SAVE_DIR, \"last.pt\"))\n",
    "    if mAP > best_map:\n",
    "        best_map = mAP\n",
    "        torch.save(ckpt, os.path.join(SAVE_DIR, \"best.pt\"))\n",
    "        print(\"💾 Saved new best checkpoint.\")\n",
    "print(f\"Done. Best mAP = {best_map:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Evaluate helper\n",
    "# =====================\n",
    "def evaluate_checkpoint(checkpoint_path):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    m = TigerDINO(dino_model=DINO_MODEL, embed_dim=EMBED_DIM, num_classes=None, pretrained=False).to(device)\n",
    "    sd = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "    m.load_state_dict(sd)\n",
    "    r1, mp = reid_eval(m, gallery_loader, query_loader, device=device)\n",
    "    print(f\"Rank-1 = {r1:.2f}% | mAP = {mp:.2f}%\")\n",
    "    return r1, mp\n",
    "\n",
    "# Example:\n",
    "# evaluate_checkpoint(os.path.join(SAVE_DIR, \"best.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
